{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using multi-agent RL with Ray RLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are going to implement a multi-agent RL based on Proximal Policy Optimization (PPO) algorithm using Ray RLlib. We are going to utilize the custom environment RobotsMeeting created in [lesson 3 nb 1](./1-exploring_multi_agent_environment.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import gymnasium.spaces as spaces\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.tune.registry import register_env\n",
    "import numpy as np\n",
    "from ray import air, tune\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "from typing import Dict\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below implements the environment for the cooperative navigation problem explained in the previous notebook [lesson 3 nb 1](./1-exploring_multi_agent_environment.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobotsMeeting(\n",
    "    MultiAgentEnv\n",
    "):  # We have to inherit from MultiAgentEnv from Ray RLlib similarly to Gymnasium API\n",
    "    def __init__(\n",
    "        self,\n",
    "        scenario_size: int = 10,\n",
    "        render: bool = False,\n",
    "    ):\n",
    "        self.scenario_size = scenario_size  # scenario_size x scenario_size grid\n",
    "        self.agents = {\"robot_\" + str(r) for r in range(2)}\n",
    "        self._agent_ids = set(self.agents)\n",
    "        self._obs_space_in_preferred_format = True\n",
    "        self._action_space_in_preferred_format = True\n",
    "        self.scenario = np.zeros((self.scenario_size, self.scenario_size))\n",
    "        self.action_space = spaces.Dict(\n",
    "            {\n",
    "                \"robot_1\": spaces.Discrete(4),  # 0: up, 1: down, 2: left, 3: right\n",
    "                \"robot_2\": spaces.Discrete(4),\n",
    "            }\n",
    "        )\n",
    "        self.step_number = 0\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                \"robot_1\": spaces.MultiDiscrete(\n",
    "                    nvec=[\n",
    "                        self.scenario_size,\n",
    "                        self.scenario_size,\n",
    "                        self.scenario_size,\n",
    "                        self.scenario_size,\n",
    "                    ]\n",
    "                ),  # Position of robot 1 and robot 2\n",
    "                \"robot_2\": spaces.MultiDiscrete(\n",
    "                    nvec=[\n",
    "                        self.scenario_size,\n",
    "                        self.scenario_size,\n",
    "                        self.scenario_size,\n",
    "                        self.scenario_size,\n",
    "                    ]\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "        self.robots = {\n",
    "            \"robot_1\": {\n",
    "                \"number\": 1,\n",
    "                \"pos\": [0, 0],\n",
    "            },\n",
    "            \"robot_2\": {\n",
    "                \"number\": 2,\n",
    "                \"pos\": [self.scenario_size - 1, self.scenario_size - 1],\n",
    "            },\n",
    "        }\n",
    "        self.scenario[0, 0] = self.robots[\"robot_1\"][\"number\"]\n",
    "        self.scenario[self.scenario_size - 1, self.scenario_size - 1] = self.robots[\n",
    "            \"robot_2\"\n",
    "        ][\"number\"]\n",
    "        if render:\n",
    "            plt.figure()\n",
    "            plt.show()\n",
    "            self.render()\n",
    "\n",
    "    def reset(self, seed, options):\n",
    "        # Reseting scenario and returning robots to initial positions\n",
    "        self.scenario = np.zeros((self.scenario_size, self.scenario_size))\n",
    "        self.scenario[0, 0] = self.robots[\"robot_1\"][\"number\"]\n",
    "        self.robots[\"robot_1\"][\"pos\"] = [0, 0]\n",
    "        self.scenario[self.scenario_size - 1, self.scenario_size - 1] = self.robots[\n",
    "            \"robot_2\"\n",
    "        ][\"number\"]\n",
    "        self.robots[\"robot_2\"][\"pos\"] = [self.scenario_size - 1, self.scenario_size - 1]\n",
    "        self.step_number = 0\n",
    "        obs = {\n",
    "            \"robot_1\": np.append(\n",
    "                self.robots[\"robot_1\"][\"pos\"], self.robots[\"robot_2\"][\"pos\"]\n",
    "            ),\n",
    "            \"robot_2\": np.append(\n",
    "                self.robots[\"robot_1\"][\"pos\"], self.robots[\"robot_2\"][\"pos\"]\n",
    "            ),\n",
    "        }\n",
    "        info = {}\n",
    "\n",
    "        return (obs, info)\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        for agent, action in action_dict.items():\n",
    "            if action == 0:  # Up\n",
    "                if self.robots[agent][\"pos\"][0] == 0:\n",
    "                    continue\n",
    "                self.scenario[\n",
    "                    self.robots[agent][\"pos\"][0], self.robots[agent][\"pos\"][1]\n",
    "                ] = 0\n",
    "                self.scenario[\n",
    "                    self.robots[agent][\"pos\"][0] - 1, self.robots[agent][\"pos\"][1]\n",
    "                ] = self.robots[agent][\"number\"]\n",
    "                self.robots[agent][\"pos\"][0] -= 1\n",
    "            elif action == 1:  # Down\n",
    "                if self.robots[agent][\"pos\"][0] == self.scenario_size - 1:\n",
    "                    continue\n",
    "                self.scenario[\n",
    "                    self.robots[agent][\"pos\"][0], self.robots[agent][\"pos\"][1]\n",
    "                ] = 0\n",
    "                self.scenario[\n",
    "                    self.robots[agent][\"pos\"][0] + 1, self.robots[agent][\"pos\"][1]\n",
    "                ] = self.robots[agent][\"number\"]\n",
    "                self.robots[agent][\"pos\"][0] += 1\n",
    "            elif action == 2:  # Left\n",
    "                if self.robots[agent][\"pos\"][1] == 0:\n",
    "                    continue\n",
    "                self.scenario[\n",
    "                    self.robots[agent][\"pos\"][0], self.robots[agent][\"pos\"][1]\n",
    "                ] = 0\n",
    "                self.scenario[\n",
    "                    self.robots[agent][\"pos\"][0], self.robots[agent][\"pos\"][1] - 1\n",
    "                ] = self.robots[agent][\"number\"]\n",
    "                self.robots[agent][\"pos\"][1] -= 1\n",
    "            elif action == 3:  # Right\n",
    "                if self.robots[agent][\"pos\"][1] == self.scenario_size - 1:\n",
    "                    continue\n",
    "                self.scenario[\n",
    "                    self.robots[agent][\"pos\"][0], self.robots[agent][\"pos\"][1]\n",
    "                ] = 0\n",
    "                self.scenario[\n",
    "                    self.robots[agent][\"pos\"][0], self.robots[agent][\"pos\"][1] + 1\n",
    "                ] = self.robots[agent][\"number\"]\n",
    "                self.robots[agent][\"pos\"][1] += 1\n",
    "            else:\n",
    "                raise ValueError(\"Invalid action\")\n",
    "        self.step_number += 1\n",
    "        distance = np.abs(  # Calculate the distance between two robots\n",
    "            self.robots[\"robot_1\"][\"pos\"][0] - self.robots[\"robot_2\"][\"pos\"][0]\n",
    "        ) + np.abs(self.robots[\"robot_1\"][\"pos\"][1] - self.robots[\"robot_2\"][\"pos\"][1])\n",
    "        obs = {\n",
    "            \"robot_1\": np.append(\n",
    "                self.robots[\"robot_1\"][\"pos\"], self.robots[\"robot_2\"][\"pos\"]\n",
    "            ),\n",
    "            \"robot_2\": np.append(\n",
    "                self.robots[\"robot_1\"][\"pos\"], self.robots[\"robot_2\"][\"pos\"]\n",
    "            ),\n",
    "        }\n",
    "        reward_value = -distance  # Reward is -distance\n",
    "        done = (\n",
    "            True if reward_value == 0 else False\n",
    "        )  # Terminate the episode if the robots meet\n",
    "        reward = {\"robot_1\": reward_value, \"robot_2\": reward_value}\n",
    "        terminated = {\"player_1\": done, \"player_2\": done}\n",
    "        truncated = {\"player_1\": done, \"player_2\": done}\n",
    "        terminated[\"__all__\"], truncated[\"__all__\"] = done, done\n",
    "        info = {}\n",
    "\n",
    "        return (obs, reward, terminated, truncated, info)\n",
    "\n",
    "    def render(self):\n",
    "        clear_output(wait=True)\n",
    "        plt.imshow(self.scenario)\n",
    "        plt.show()\n",
    "\n",
    "    def close(self):\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Registering our custom environment in the Ray RLlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_creator(env_config):\n",
    "    env = RobotsMeeting(scenario_size=10, render=False)\n",
    "    return env\n",
    "\n",
    "\n",
    "register_env(\"robots_meeting\", lambda config: env_creator(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the multi-agent RL using PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is the responsible to create two different policies (one for each robot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_policies() -> Dict[str, PolicySpec]:\n",
    "    policies = {\n",
    "        \"robot_1\": PolicySpec(),\n",
    "        \"robot_2\": PolicySpec(),\n",
    "    }\n",
    "\n",
    "    return policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below associate each robot for a specific policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_mapping_fn(agent_id, episode=None, worker=None, **kwargs):\n",
    "    agent_idx = int(agent_id.partition(\"_\")[2])\n",
    "\n",
    "    return \"robot_1\" if agent_idx == 1 else \"robot_2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can create the MARL agent using the `multi_agent` callback from `PPOConfig` where we pass the functions to create and associate policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_config = (\n",
    "    PPOConfig()\n",
    "    .environment(\n",
    "        env=\"robots_meeting\",\n",
    "    )\n",
    "    .multi_agent(\n",
    "        policies=generate_policies(),\n",
    "        policy_mapping_fn=policy_mapping_fn,\n",
    "        count_steps_by=\"env_steps\",\n",
    "    )\n",
    "    .training(\n",
    "        train_batch_size=200,\n",
    "        sgd_minibatch_size=50,\n",
    "    )\n",
    "    .framework(\"torch\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process implemented in the cell below is exactly the same used in a single-agent training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 15:21:37,480\tINFO worker.py:1783 -- Started a local Ray instance.\n",
      "2024-12-07 15:21:37,998\tINFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n",
      "2024-12-07 15:21:38,000\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "/home/lasse/.local/share/virtualenvs/ray_minicourse-PTDOXG61/lib/python3.10/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "/home/lasse/.local/share/virtualenvs/ray_minicourse-PTDOXG61/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:164: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/home/lasse/.local/share/virtualenvs/ray_minicourse-PTDOXG61/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:188: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-12-07 15:23:10</td></tr>\n",
       "<tr><td>Running for: </td><td>00:01:32.65        </td></tr>\n",
       "<tr><td>Memory:      </td><td>5.9/23.9 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 3.0/16 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status    </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_in_flight_async_\n",
       "sample_reqs</th><th style=\"text-align: right;\">  num_remote_worker_re\n",
       "starts</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_robots_meeting_f3d9b_00000</td><td>TERMINATED</td><td>200.239.93.233:695821</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         83.4747</td><td style=\"text-align: right;\">10000</td><td style=\"text-align: right;\">                    2</td><td style=\"text-align: right;\">0</td><td style=\"text-align: right;\">0</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO pid=695821)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[36m(PPO pid=695821)\u001b[0m 2024-12-07 15:21:46,478\tWARNING deprecation.py:50 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th style=\"text-align: right;\">  agent_timesteps_total</th><th>counters                                                                                                                            </th><th>custom_metrics  </th><th>env_runners                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          </th><th>episode_media  </th><th>info                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          </th><th style=\"text-align: right;\">  num_agent_steps_sampled</th><th style=\"text-align: right;\">  num_agent_steps_sampled_lifetime</th><th style=\"text-align: right;\">  num_agent_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_sampled</th><th style=\"text-align: right;\">  num_env_steps_sampled_lifetime</th><th style=\"text-align: right;\">  num_env_steps_sampled_this_iter</th><th style=\"text-align: right;\">  num_env_steps_sampled_throughput_per_sec</th><th style=\"text-align: right;\">  num_env_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_trained_this_iter</th><th style=\"text-align: right;\">  num_env_steps_trained_throughput_per_sec</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_in_flight_async_sample_reqs</th><th style=\"text-align: right;\">  num_remote_worker_restarts</th><th style=\"text-align: right;\">  num_steps_trained_this_iter</th><th>perf                                                             </th><th>timers                                                                                                                                                                                                                         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_robots_meeting_f3d9b_00000</td><td style=\"text-align: right;\">                  20000</td><td>{&#x27;num_env_steps_sampled&#x27;: 10000, &#x27;num_env_steps_trained&#x27;: 10000, &#x27;num_agent_steps_sampled&#x27;: 20000, &#x27;num_agent_steps_trained&#x27;: 20000}</td><td>{}              </td><td>{&#x27;episode_reward_max&#x27;: -144.0, &#x27;episode_reward_min&#x27;: -524.0, &#x27;episode_reward_mean&#x27;: -223.32, &#x27;episode_len_mean&#x27;: 19.74, &#x27;episode_media&#x27;: {}, &#x27;episodes_timesteps_total&#x27;: 1974, &#x27;policy_reward_min&#x27;: {&#x27;robot_1&#x27;: -262.0, &#x27;robot_2&#x27;: -262.0}, &#x27;policy_reward_max&#x27;: {&#x27;robot_1&#x27;: -72.0, &#x27;robot_2&#x27;: -72.0}, &#x27;policy_reward_mean&#x27;: {&#x27;robot_1&#x27;: -111.66, &#x27;robot_2&#x27;: -111.66}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [-154.0, -274.0, -212.0, -204.0, -228.0, -190.0, -264.0, -524.0, -414.0, -186.0, -158.0, -156.0, -204.0, -144.0, -210.0, -148.0, -300.0, -190.0, -194.0, -224.0, -264.0, -242.0, -218.0, -300.0, -294.0, -302.0, -178.0, -298.0, -224.0, -144.0, -216.0, -144.0, -244.0, -410.0, -154.0, -152.0, -170.0, -170.0, -144.0, -216.0, -144.0, -256.0, -164.0, -144.0, -200.0, -192.0, -242.0, -246.0, -172.0, -144.0, -230.0, -174.0, -444.0, -248.0, -260.0, -162.0, -144.0, -166.0, -224.0, -170.0, -240.0, -144.0, -216.0, -192.0, -160.0, -144.0, -208.0, -252.0, -144.0, -206.0, -234.0, -188.0, -214.0, -248.0, -214.0, -210.0, -144.0, -392.0, -164.0, -318.0, -182.0, -144.0, -180.0, -160.0, -202.0, -194.0, -208.0, -236.0, -208.0, -418.0, -156.0, -316.0, -204.0, -232.0, -204.0, -216.0, -494.0, -260.0, -456.0, -242.0], &#x27;episode_lengths&#x27;: [12, 27, 23, 15, 26, 17, 19, 48, 36, 16, 11, 10, 12, 9, 23, 10, 30, 21, 18, 21, 22, 27, 18, 31, 25, 29, 16, 37, 24, 9, 23, 9, 17, 38, 12, 10, 13, 13, 9, 19, 9, 19, 10, 9, 24, 18, 27, 28, 11, 9, 19, 13, 43, 25, 27, 11, 9, 11, 24, 13, 27, 9, 12, 20, 13, 9, 22, 25, 9, 21, 24, 20, 19, 23, 22, 20, 9, 40, 10, 32, 15, 9, 16, 10, 21, 18, 19, 21, 17, 32, 10, 27, 17, 20, 19, 12, 60, 24, 39, 18], &#x27;policy_robot_1_reward&#x27;: [-77.0, -137.0, -106.0, -102.0, -114.0, -95.0, -132.0, -262.0, -207.0, -93.0, -79.0, -78.0, -102.0, -72.0, -105.0, -74.0, -150.0, -95.0, -97.0, -112.0, -132.0, -121.0, -109.0, -150.0, -147.0, -151.0, -89.0, -149.0, -112.0, -72.0, -108.0, -72.0, -122.0, -205.0, -77.0, -76.0, -85.0, -85.0, -72.0, -108.0, -72.0, -128.0, -82.0, -72.0, -100.0, -96.0, -121.0, -123.0, -86.0, -72.0, -115.0, -87.0, -222.0, -124.0, -130.0, -81.0, -72.0, -83.0, -112.0, -85.0, -120.0, -72.0, -108.0, -96.0, -80.0, -72.0, -104.0, -126.0, -72.0, -103.0, -117.0, -94.0, -107.0, -124.0, -107.0, -105.0, -72.0, -196.0, -82.0, -159.0, -91.0, -72.0, -90.0, -80.0, -101.0, -97.0, -104.0, -118.0, -104.0, -209.0, -78.0, -158.0, -102.0, -116.0, -102.0, -108.0, -247.0, -130.0, -228.0, -121.0], &#x27;policy_robot_2_reward&#x27;: [-77.0, -137.0, -106.0, -102.0, -114.0, -95.0, -132.0, -262.0, -207.0, -93.0, -79.0, -78.0, -102.0, -72.0, -105.0, -74.0, -150.0, -95.0, -97.0, -112.0, -132.0, -121.0, -109.0, -150.0, -147.0, -151.0, -89.0, -149.0, -112.0, -72.0, -108.0, -72.0, -122.0, -205.0, -77.0, -76.0, -85.0, -85.0, -72.0, -108.0, -72.0, -128.0, -82.0, -72.0, -100.0, -96.0, -121.0, -123.0, -86.0, -72.0, -115.0, -87.0, -222.0, -124.0, -130.0, -81.0, -72.0, -83.0, -112.0, -85.0, -120.0, -72.0, -108.0, -96.0, -80.0, -72.0, -104.0, -126.0, -72.0, -103.0, -117.0, -94.0, -107.0, -124.0, -107.0, -105.0, -72.0, -196.0, -82.0, -159.0, -91.0, -72.0, -90.0, -80.0, -101.0, -97.0, -104.0, -118.0, -104.0, -209.0, -78.0, -158.0, -102.0, -116.0, -102.0, -108.0, -247.0, -130.0, -228.0, -121.0]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.4229939711323806, &#x27;mean_inference_ms&#x27;: 1.3427328916835284, &#x27;mean_action_processing_ms&#x27;: 0.1347682571466859, &#x27;mean_env_wait_ms&#x27;: 0.05361522725101051, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;ObsPreprocessorConnector_ms&#x27;: 0.023378849029541016, &#x27;StateBufferConnector_ms&#x27;: 0.0033608675003051758, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.06606578826904297}, &#x27;num_episodes&#x27;: 9, &#x27;episode_return_max&#x27;: -144.0, &#x27;episode_return_min&#x27;: -524.0, &#x27;episode_return_mean&#x27;: -223.32, &#x27;episodes_this_iter&#x27;: 9}</td><td>{}             </td><td>{&#x27;learner&#x27;: {&#x27;robot_2&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;grad_gnorm&#x27;: 1.5936879495779672, &#x27;cur_kl_coeff&#x27;: 0.00021972656250000006, &#x27;cur_lr&#x27;: 5.0000000000000016e-05, &#x27;total_loss&#x27;: 8.334545095761618, &#x27;policy_loss&#x27;: -0.04041279072698671, &#x27;vf_loss&#x27;: 8.37495540380478, &#x27;vf_explained_var&#x27;: -0.0040723904967308044, &#x27;kl&#x27;: 0.01133483401901021, &#x27;entropy&#x27;: 0.6955343385537466, &#x27;entropy_coeff&#x27;: 0.0}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 50.0, &#x27;num_grad_updates_lifetime&#x27;: 5940.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 59.5}, &#x27;robot_1&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;grad_gnorm&#x27;: 2.3928737143675485, &#x27;cur_kl_coeff&#x27;: 5.4931640625000015e-05, &#x27;cur_lr&#x27;: 5.0000000000000016e-05, &#x27;total_loss&#x27;: 8.24058399995168, &#x27;policy_loss&#x27;: -0.03297920865091631, &#x27;vf_loss&#x27;: 8.273562939961751, &#x27;vf_explained_var&#x27;: 0.021410337587197622, &#x27;kl&#x27;: 0.005922839306840611, &#x27;entropy&#x27;: 0.4649967849254608, &#x27;entropy_coeff&#x27;: 0.0}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 50.0, &#x27;num_grad_updates_lifetime&#x27;: 5940.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 59.5}}, &#x27;num_env_steps_sampled&#x27;: 10000, &#x27;num_env_steps_trained&#x27;: 10000, &#x27;num_agent_steps_sampled&#x27;: 20000, &#x27;num_agent_steps_trained&#x27;: 20000}</td><td style=\"text-align: right;\">                    20000</td><td style=\"text-align: right;\">                             20000</td><td style=\"text-align: right;\">                    20000</td><td style=\"text-align: right;\">                  10000</td><td style=\"text-align: right;\">                           10000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                                   120.427</td><td style=\"text-align: right;\">                  10000</td><td style=\"text-align: right;\">                              200</td><td style=\"text-align: right;\">                                   120.427</td><td style=\"text-align: right;\">                    2</td><td style=\"text-align: right;\">                                0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                          200</td><td>{&#x27;cpu_util_percent&#x27;: 7.233333333333333, &#x27;ram_util_percent&#x27;: 24.7}</td><td>{&#x27;training_iteration_time_ms&#x27;: 1650.607, &#x27;restore_workers_time_ms&#x27;: 0.014, &#x27;training_step_time_ms&#x27;: 1650.564, &#x27;sample_time_ms&#x27;: 202.261, &#x27;learn_time_ms&#x27;: 1444.17, &#x27;learn_throughput&#x27;: 138.488, &#x27;synch_weights_time_ms&#x27;: 3.972}</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 15:23:10,687\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/lasse/ray_minicourse/lesson_3/ray_results/nb_2/ray_ppo' in 0.0128s.\n",
      "\u001b[36m(PPO pid=695821)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/lasse/ray_minicourse/lesson_3/ray_results/nb_2/ray_ppo/PPO_robots_meeting_f3d9b_00000_0_2024-12-07_15-21-38/checkpoint_000000)\n",
      "2024-12-07 15:23:11,299\tINFO tune.py:1041 -- Total run time: 93.30 seconds (92.64 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResultGrid<[\n",
      "  Result(\n",
      "    metrics={'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'robot_2': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 1.5936879495779672, 'cur_kl_coeff': 0.00021972656250000006, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 8.334545095761618, 'policy_loss': -0.04041279072698671, 'vf_loss': 8.37495540380478, 'vf_explained_var': -0.0040723904967308044, 'kl': 0.01133483401901021, 'entropy': 0.6955343385537466, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 50.0, 'num_grad_updates_lifetime': 5940.5, 'diff_num_grad_updates_vs_sampler_policy': 59.5}, 'robot_1': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 2.3928737143675485, 'cur_kl_coeff': 5.4931640625000015e-05, 'cur_lr': 5.0000000000000016e-05, 'total_loss': 8.24058399995168, 'policy_loss': -0.03297920865091631, 'vf_loss': 8.273562939961751, 'vf_explained_var': 0.021410337587197622, 'kl': 0.005922839306840611, 'entropy': 0.4649967849254608, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 50.0, 'num_grad_updates_lifetime': 5940.5, 'diff_num_grad_updates_vs_sampler_policy': 59.5}}, 'num_env_steps_sampled': 10000, 'num_env_steps_trained': 10000, 'num_agent_steps_sampled': 20000, 'num_agent_steps_trained': 20000}, 'env_runners': {'episode_reward_max': -144.0, 'episode_reward_min': -524.0, 'episode_reward_mean': -223.32, 'episode_len_mean': 19.74, 'episode_media': {}, 'episodes_timesteps_total': 1974, 'policy_reward_min': {'robot_1': -262.0, 'robot_2': -262.0}, 'policy_reward_max': {'robot_1': -72.0, 'robot_2': -72.0}, 'policy_reward_mean': {'robot_1': -111.66, 'robot_2': -111.66}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-154.0, -274.0, -212.0, -204.0, -228.0, -190.0, -264.0, -524.0, -414.0, -186.0, -158.0, -156.0, -204.0, -144.0, -210.0, -148.0, -300.0, -190.0, -194.0, -224.0, -264.0, -242.0, -218.0, -300.0, -294.0, -302.0, -178.0, -298.0, -224.0, -144.0, -216.0, -144.0, -244.0, -410.0, -154.0, -152.0, -170.0, -170.0, -144.0, -216.0, -144.0, -256.0, -164.0, -144.0, -200.0, -192.0, -242.0, -246.0, -172.0, -144.0, -230.0, -174.0, -444.0, -248.0, -260.0, -162.0, -144.0, -166.0, -224.0, -170.0, -240.0, -144.0, -216.0, -192.0, -160.0, -144.0, -208.0, -252.0, -144.0, -206.0, -234.0, -188.0, -214.0, -248.0, -214.0, -210.0, -144.0, -392.0, -164.0, -318.0, -182.0, -144.0, -180.0, -160.0, -202.0, -194.0, -208.0, -236.0, -208.0, -418.0, -156.0, -316.0, -204.0, -232.0, -204.0, -216.0, -494.0, -260.0, -456.0, -242.0], 'episode_lengths': [12, 27, 23, 15, 26, 17, 19, 48, 36, 16, 11, 10, 12, 9, 23, 10, 30, 21, 18, 21, 22, 27, 18, 31, 25, 29, 16, 37, 24, 9, 23, 9, 17, 38, 12, 10, 13, 13, 9, 19, 9, 19, 10, 9, 24, 18, 27, 28, 11, 9, 19, 13, 43, 25, 27, 11, 9, 11, 24, 13, 27, 9, 12, 20, 13, 9, 22, 25, 9, 21, 24, 20, 19, 23, 22, 20, 9, 40, 10, 32, 15, 9, 16, 10, 21, 18, 19, 21, 17, 32, 10, 27, 17, 20, 19, 12, 60, 24, 39, 18], 'policy_robot_1_reward': [-77.0, -137.0, -106.0, -102.0, -114.0, -95.0, -132.0, -262.0, -207.0, -93.0, -79.0, -78.0, -102.0, -72.0, -105.0, -74.0, -150.0, -95.0, -97.0, -112.0, -132.0, -121.0, -109.0, -150.0, -147.0, -151.0, -89.0, -149.0, -112.0, -72.0, -108.0, -72.0, -122.0, -205.0, -77.0, -76.0, -85.0, -85.0, -72.0, -108.0, -72.0, -128.0, -82.0, -72.0, -100.0, -96.0, -121.0, -123.0, -86.0, -72.0, -115.0, -87.0, -222.0, -124.0, -130.0, -81.0, -72.0, -83.0, -112.0, -85.0, -120.0, -72.0, -108.0, -96.0, -80.0, -72.0, -104.0, -126.0, -72.0, -103.0, -117.0, -94.0, -107.0, -124.0, -107.0, -105.0, -72.0, -196.0, -82.0, -159.0, -91.0, -72.0, -90.0, -80.0, -101.0, -97.0, -104.0, -118.0, -104.0, -209.0, -78.0, -158.0, -102.0, -116.0, -102.0, -108.0, -247.0, -130.0, -228.0, -121.0], 'policy_robot_2_reward': [-77.0, -137.0, -106.0, -102.0, -114.0, -95.0, -132.0, -262.0, -207.0, -93.0, -79.0, -78.0, -102.0, -72.0, -105.0, -74.0, -150.0, -95.0, -97.0, -112.0, -132.0, -121.0, -109.0, -150.0, -147.0, -151.0, -89.0, -149.0, -112.0, -72.0, -108.0, -72.0, -122.0, -205.0, -77.0, -76.0, -85.0, -85.0, -72.0, -108.0, -72.0, -128.0, -82.0, -72.0, -100.0, -96.0, -121.0, -123.0, -86.0, -72.0, -115.0, -87.0, -222.0, -124.0, -130.0, -81.0, -72.0, -83.0, -112.0, -85.0, -120.0, -72.0, -108.0, -96.0, -80.0, -72.0, -104.0, -126.0, -72.0, -103.0, -117.0, -94.0, -107.0, -124.0, -107.0, -105.0, -72.0, -196.0, -82.0, -159.0, -91.0, -72.0, -90.0, -80.0, -101.0, -97.0, -104.0, -118.0, -104.0, -209.0, -78.0, -158.0, -102.0, -116.0, -102.0, -108.0, -247.0, -130.0, -228.0, -121.0]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.4229939711323806, 'mean_inference_ms': 1.3427328916835284, 'mean_action_processing_ms': 0.1347682571466859, 'mean_env_wait_ms': 0.05361522725101051, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.023378849029541016, 'StateBufferConnector_ms': 0.0033608675003051758, 'ViewRequirementAgentConnector_ms': 0.06606578826904297}, 'num_episodes': 9, 'episode_return_max': -144.0, 'episode_return_min': -524.0, 'episode_return_mean': -223.32, 'episodes_this_iter': 9}, 'num_healthy_workers': 2, 'num_in_flight_async_sample_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 20000, 'num_agent_steps_trained': 20000, 'num_env_steps_sampled': 10000, 'num_env_steps_trained': 10000, 'num_env_steps_sampled_this_iter': 200, 'num_env_steps_trained_this_iter': 200, 'num_env_steps_sampled_throughput_per_sec': 120.42728200911208, 'num_env_steps_trained_throughput_per_sec': 120.42728200911208, 'num_env_steps_sampled_lifetime': 10000, 'num_agent_steps_sampled_lifetime': 20000, 'num_steps_trained_this_iter': 200, 'agent_timesteps_total': 20000, 'timers': {'training_iteration_time_ms': 1650.607, 'restore_workers_time_ms': 0.014, 'training_step_time_ms': 1650.564, 'sample_time_ms': 202.261, 'learn_time_ms': 1444.17, 'learn_throughput': 138.488, 'synch_weights_time_ms': 3.972}, 'counters': {'num_env_steps_sampled': 10000, 'num_env_steps_trained': 10000, 'num_agent_steps_sampled': 20000, 'num_agent_steps_trained': 20000}, 'perf': {'cpu_util_percent': 7.233333333333333, 'ram_util_percent': 24.7}},\n",
      "    path='/home/lasse/ray_minicourse/lesson_3/ray_results/nb_2/ray_ppo/PPO_robots_meeting_f3d9b_00000_0_2024-12-07_15-21-38',\n",
      "    filesystem='local',\n",
      "    checkpoint=Checkpoint(filesystem=local, path=/home/lasse/ray_minicourse/lesson_3/ray_results/nb_2/ray_ppo/PPO_robots_meeting_f3d9b_00000_0_2024-12-07_15-21-38/checkpoint_000000)\n",
      "  )\n",
      "]>\n"
     ]
    }
   ],
   "source": [
    "stop = {\n",
    "    \"num_env_steps_sampled\": 10000,  # 10000 environment steps\n",
    "}\n",
    "checkpoint_frequency = 0\n",
    "store_results_path = str(Path(\"./ray_results/\").resolve()) + \"/nb_2/\"\n",
    "agent_name = \"ray_ppo\"\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    \"PPO\",\n",
    "    param_space=algo_config.to_dict(),\n",
    "    run_config=air.RunConfig(\n",
    "        storage_path=store_results_path,\n",
    "        name=agent_name,\n",
    "        stop=stop,\n",
    "        verbose=2,\n",
    "        checkpoint_config=air.CheckpointConfig(\n",
    "            checkpoint_frequency=checkpoint_frequency,\n",
    "            checkpoint_at_end=True,\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "results = tuner.fit()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 15:23:11,361\tWARNING deprecation.py:50 -- DeprecationWarning: `_enable_new_api_stack` has been deprecated. Use `AlgorithmConfig._enable_new_api_stack` instead. This will raise an error in the future!\n",
      "/home/lasse/.local/share/virtualenvs/ray_minicourse-PTDOXG61/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:557: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/lasse/.local/share/virtualenvs/ray_minicourse-PTDOXG61/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/lasse/.local/share/virtualenvs/ray_minicourse-PTDOXG61/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/lasse/.local/share/virtualenvs/ray_minicourse-PTDOXG61/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2024-12-07 15:23:15,993\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "analysis = tune.ExperimentAnalysis(f\"{store_results_path}/{agent_name}/\")\n",
    "assert analysis.trials is not None, \"Analysis trial is None\"\n",
    "checkpoint = analysis.get_best_checkpoint(\n",
    "    trial=analysis.trials[0], metric=\"env_runners/episode_reward_mean\", mode=\"max\"\n",
    ")\n",
    "tuner_agent = Algorithm.from_checkpoint(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's test our trained agents in an episode of the environment. There is a small difference to compose the agent action since now we utilize two different RL agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "env = env_creator({})\n",
    "obs, _ = env.reset(seed=0, options={})  # Initial observation\n",
    "terminated = {\"__all__\": False}\n",
    "number_steps = 0\n",
    "while not terminated[\"__all__\"]:\n",
    "    action = {}\n",
    "    assert isinstance(obs, dict), \"Observations must be a dictionary.\"\n",
    "    for (\n",
    "        agent_id,\n",
    "        agent_obs,\n",
    "    ) in (\n",
    "        obs.items()\n",
    "    ):  # We need to iterate for each agent to compose the action (dictionary)\n",
    "        policy_id = policy_mapping_fn(agent_id)\n",
    "        action[agent_id] = tuner_agent.compute_single_action(\n",
    "            agent_obs,\n",
    "            policy_id=policy_id,\n",
    "            explore=True,\n",
    "        )\n",
    "    obs, reward, terminated, truncated, info = env.step(\n",
    "        action\n",
    "    )  # Applying the action in the environment\n",
    "    number_steps += 1\n",
    "    env.render()  # Rendering the environment\n",
    "print(number_steps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray_minicourse-PTDOXG61",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
