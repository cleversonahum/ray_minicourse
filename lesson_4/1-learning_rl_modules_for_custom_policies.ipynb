{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding RLModules for custom policies in Ray RLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from ray.rllib.core.rl_module.rl_module import RLModuleSpec\n",
    "from typing import Any, Dict\n",
    "from ray.rllib.core.rl_module.torch.torch_rl_module import TorchRLModule\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ray RLlib defines [RLModules](https://docs.ray.io/en/latest/rllib/key-concepts.html) as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> RLModules are framework-specific neural network containers. In a nutshell, they carry the neural networks and define how to use them during three phases that occur in reinforcement learning: Exploration, inference and training. A minimal RL Module can contain a single neural network and define its exploration-, inference- and training logic to only map observations to actions. Since RL Modules can map observations to actions, they naturally implement reinforcement learning policies in RLlib and can therefore be found in the RolloutWorker, where their exploration and inference logic is used to sample from an environment. The second place in RLlib where RL Modules commonly occur is the Learner, where their training logic is used in training the neural network. RL Modules extend to the multi-agent case, where a single MultiRLModule contains multiple RL Modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarizing, the RLModules implement the neural networks our RL algorithms are going to use for training and inferencing. So, in order to implement our own neural network architectures and input/output policy dynamics, it is essential to understand RLModules. Figure below illustrates the RLModules being responsible for dealing the neural network inputs and outputs.\n",
    "\n",
    "![RLModules archtecture](./imgs/rl_module.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's investigate the RLModule structure by implementing a custom RLModule for a discrete action space based on Torch framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteTorchModule(TorchRLModule):  # We inherit from base class TorchRLModule\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space,\n",
    "        action_space,\n",
    "        inference_only,\n",
    "        model_config,\n",
    "        catalog_class,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            observation_space=observation_space,\n",
    "            action_space=action_space,\n",
    "            inference_only=inference_only,\n",
    "            model_config=model_config,\n",
    "            catalog_class=catalog_class,\n",
    "        )\n",
    "\n",
    "    def setup(self):\n",
    "        # Here we are going to create the policy network (neural network)\n",
    "        input_dim = self.observation_space.shape[\n",
    "            0\n",
    "        ]  # The neural network input dimension is the same as the observation space dimension\n",
    "        hidden_dim = self.model_config[\"fcnet_hiddens\"][\n",
    "            0\n",
    "        ]  # Represents the number of hidden units in the neural network\n",
    "        output_dim = (\n",
    "            self.action_space.n\n",
    "        )  # Finally, we have one neuron (output) per action in the action space\n",
    "\n",
    "        self.policy = nn.Sequential(  # Here we create the neural network using PyTorch\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "        )\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "    def _forward_inference(self, batch: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        # Forward pass for training.\n",
    "        with torch.no_grad():  # It disables gradient calculation and therefore no weight updates could be applied (and we don't want it since we are not training)\n",
    "            action = np.argmax(self.policy(batch[\"obs\"]))\n",
    "            return {\"actions\": action}\n",
    "\n",
    "    def _forward_exploration(self, batch: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        exploration_epsilon = 0.2\n",
    "        if np.random.rand() < exploration_epsilon:\n",
    "            # Random action exploration\n",
    "            action = np.random.choice(self.action_space.n)\n",
    "            return {\"actions\": action}\n",
    "        else:\n",
    "            # Use logits from policy network for action selection\n",
    "            logits = self.policy(batch[\"obs\"])\n",
    "            return {\n",
    "                \"action_dist_inputs\": torch.distributions.Categorical(logits=logits)\n",
    "            }\n",
    "\n",
    "    def _forward_train(self, batch: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        # Forward pass for training\n",
    "        action_logits = self.policy(\n",
    "            batch[\"obs\"]\n",
    "        )  # Here we pass the observation to the neural network and accounts the gradients\n",
    "        # Be aware that the gradients are accounted here because we are training\n",
    "\n",
    "        # When using a discrete action space, the neural network output is not the action itself, but the logits of the action distribution, therefore we need to apply a softmax function to get the discrete action. Therefore, we have one neural network output per action in the action space.\n",
    "        return {\n",
    "            \"action_dist_inputs\": torch.distributions.Categorical(logits=action_logits)\n",
    "        }  # You can read more about the Categorical distribution in the PyTorch documentation here https://pytorch.org/docs/stable/distributions.html#categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `RLModuleSpec` class to build our final RLModule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-14 03:33:43,698\tWARNING rl_module.py:427 -- Could not create a Catalog object for your RLModule! If you are not using the new API stack yet, make sure to switch it off in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. Some algos already use the new stack by default. Ignore this message, if your RLModule does not use a Catalog to build its sub-components.\n",
      "2024-12-14 03:33:43,699\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "spec = RLModuleSpec(\n",
    "    module_class=DiscreteTorchModule,\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    model_config={\"fcnet_hiddens\": [64]},\n",
    ")\n",
    "\n",
    "rlmodule = spec.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have a variable `rlmodule` containing a RLModule to interact with our neural network. It was not trained before, so the neural network weights have their initial values intact. Let's interact with it to verify how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inference does not explore new possibilities and, instead, always choose the action with the highest logit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs: tensor([1.3627, 0.1826, 0.1172, 0.1369])\n",
      "Action: 1\n"
     ]
    }
   ],
   "source": [
    "batch = {\"obs\": torch.from_numpy(env.observation_space.sample())}\n",
    "print(f\"Obs: {batch['obs']}\")\n",
    "\n",
    "# Forward inference\n",
    "inference = rlmodule.forward_inference(batch)\n",
    "inference_actions = inference[\"actions\"]\n",
    "print(f\"Action: {inference_actions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our exploration function explores random actions with a 20% of chance, otherwise it chooses the action with the highest logit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action dist: {'action_dist_inputs': Categorical(logits: torch.Size([2]))}\n",
      "Logits: tensor([-0.8223, -0.5788], grad_fn=<SubBackward0>)\n",
      "Probabilities: tensor([0.4394, 0.5606], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Forward exploration\n",
    "exploration = rlmodule.forward_exploration(batch)\n",
    "if \"action\" in exploration:\n",
    "    print(f\"Action: {exploration['actions']}\")\n",
    "elif \"action_dist_inputs\" in exploration:\n",
    "    exploration_action_dist_inputs = exploration[\"action_dist_inputs\"]\n",
    "    print(f\"Action dist: {exploration}\")\n",
    "    print(f\"Logits: {exploration_action_dist_inputs.logits}\")\n",
    "    print(f\"Probabilities: {exploration_action_dist_inputs.probs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training, we return the distribution of probabilities for each action with gradient calculation enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 1\n",
      "Logits: tensor([-0.8223, -0.5788], grad_fn=<SubBackward0>)\n",
      "Probabilities: tensor([0.4394, 0.5606], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Forward train\n",
    "train = rlmodule.forward_train(batch)\n",
    "train_action_dist_inputs = train[\"action_dist_inputs\"]\n",
    "print(f\"Action: {train_action_dist_inputs.sample()}\")\n",
    "print(f\"Logits: {train_action_dist_inputs.logits}\")\n",
    "print(f\"Probabilities: {train_action_dist_inputs.probs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's now make a quick look at PPO RLModule. Remember that PPO is an actor-critic method and therefore utilizes two neural networks, one for the actor (phi) and another for the critic (vf). See the Ray RLlib implementation for [PPO RLModule](https://github.com/ray-project/ray/blob/master/rllib/algorithms/ppo/ppo_rl_module.py) below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file holds framework-agnostic components for PPO's RLModules.\n",
    "\"\"\"\n",
    "\n",
    "import abc\n",
    "from typing import List\n",
    "\n",
    "from ray.rllib.core.models.configs import RecurrentEncoderConfig\n",
    "from ray.rllib.core.rl_module.apis import InferenceOnlyAPI, ValueFunctionAPI\n",
    "from ray.rllib.core.rl_module.rl_module import RLModule\n",
    "from ray.rllib.utils.annotations import (\n",
    "    override,\n",
    "    OverrideToImplementCustomLogic_CallToSuperRecommended,\n",
    ")\n",
    "from ray.util.annotations import DeveloperAPI\n",
    "\n",
    "\n",
    "@DeveloperAPI(stability=\"alpha\")\n",
    "class PPORLModule(RLModule, InferenceOnlyAPI, ValueFunctionAPI, abc.ABC):\n",
    "    @override(RLModule)\n",
    "    def setup(self):\n",
    "        if self.catalog is None and hasattr(self, \"_catalog_ctor_error\"):\n",
    "            raise self._catalog_ctor_error\n",
    "\n",
    "        is_stateful = isinstance(\n",
    "            self.catalog.actor_critic_encoder_config.base_encoder_config,\n",
    "            RecurrentEncoderConfig,\n",
    "        )\n",
    "        if is_stateful:\n",
    "            self.inference_only = False\n",
    "\n",
    "        if self.inference_only and self.framework == \"torch\":\n",
    "            self.catalog.actor_critic_encoder_config.inference_only = True\n",
    "\n",
    "        # Build models from catalog.\n",
    "        # Here we build the pi and vf policies (neural networks) to be used by the PPO algorithm\n",
    "        self.encoder = self.catalog.build_actor_critic_encoder(framework=self.framework)\n",
    "        self.pi = self.catalog.build_pi_head(framework=self.framework)\n",
    "        self.vf = self.catalog.build_vf_head(framework=self.framework)\n",
    "        # __sphinx_doc_end__\n",
    "\n",
    "    @override(RLModule)\n",
    "    def get_initial_state(self) -> dict:\n",
    "        if hasattr(self.encoder, \"get_initial_state\"):\n",
    "            return self.encoder.get_initial_state()\n",
    "        else:\n",
    "            return {}\n",
    "\n",
    "    @OverrideToImplementCustomLogic_CallToSuperRecommended\n",
    "    @override(InferenceOnlyAPI)\n",
    "    def get_non_inference_attributes(self) -> List[str]:\n",
    "        \"\"\"Return attributes, which are NOT inference-only (only used for training).\"\"\"\n",
    "        return [\"vf\"] + (\n",
    "            []\n",
    "            if self.model_config.get(\"vf_share_layers\")\n",
    "            else [\"encoder.critic_encoder\"]\n",
    "        )\n",
    "\n",
    "    # Where are the functions _forward_inference, _forward_exploration and _forward_train?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pay attention that the PPORLModule does not implement the _forward_inference(), _forward_exploration() and _forward_train() methods. This is because the PPORLModule is a base class for the PPO algorithm, but the forward mechanism is coupled to the framework to compute gradients used in Ray RLlib: Pytorch and Tensorflow. Therefore, there is a class [PPOTorchRLModule](https://github.com/ray-project/ray/blob/master/rllib/algorithms/ppo/torch/ppo_torch_rl_module.py) responsible for inheriting `PPORLModule` and implementing the functions `_forward_inference()`, `_forward_exploration()` and `_forward_train()` using Pytorch specific commands. The code for class `PPOTorchRLModule` is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Optional\n",
    "\n",
    "from ray.rllib.algorithms.ppo.ppo_rl_module import PPORLModule\n",
    "from ray.rllib.core.columns import Columns\n",
    "from ray.rllib.core.models.base import ACTOR, CRITIC, ENCODER_OUT\n",
    "from ray.rllib.core.rl_module.apis.value_function_api import ValueFunctionAPI\n",
    "from ray.rllib.core.rl_module.rl_module import RLModule\n",
    "from ray.rllib.core.rl_module.torch import TorchRLModule\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from ray.rllib.utils.typing import TensorType\n",
    "\n",
    "torch, nn = try_import_torch()\n",
    "\n",
    "\n",
    "class PPOTorchRLModule(TorchRLModule, PPORLModule):\n",
    "    # We don't need to implement the setup method since it is already implemented in the PPORLModule class\n",
    "    @override(RLModule)\n",
    "    def _forward(self, batch: Dict[str, Any], **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Default forward pass (used for inference and exploration).\"\"\"\n",
    "        # Here, it uses the _forward common method to substitute the _forward_inference and _forward_exploration methods\n",
    "        output = {}\n",
    "        # Encoder forward pass.\n",
    "        encoder_outs = self.encoder(batch)\n",
    "        # Stateful encoder?\n",
    "        if Columns.STATE_OUT in encoder_outs:\n",
    "            output[Columns.STATE_OUT] = encoder_outs[Columns.STATE_OUT]\n",
    "        # Pi head.\n",
    "        output[Columns.ACTION_DIST_INPUTS] = self.pi(\n",
    "            encoder_outs[ENCODER_OUT][ACTOR]\n",
    "        )  # The forward uses the pi policy\n",
    "        return output\n",
    "\n",
    "    @override(RLModule)\n",
    "    def _forward_train(self, batch: Dict[str, Any], **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Train forward pass (keep embeddings for possible shared value func. call).\"\"\"\n",
    "        output = {}\n",
    "        encoder_outs = self.encoder(batch)\n",
    "        output[Columns.EMBEDDINGS] = encoder_outs[ENCODER_OUT][CRITIC]\n",
    "        if Columns.STATE_OUT in encoder_outs:\n",
    "            output[Columns.STATE_OUT] = encoder_outs[Columns.STATE_OUT]\n",
    "        output[Columns.ACTION_DIST_INPUTS] = self.pi(encoder_outs[ENCODER_OUT][ACTOR])\n",
    "        return output\n",
    "\n",
    "    @override(ValueFunctionAPI)\n",
    "    def compute_values(\n",
    "        self,\n",
    "        batch: Dict[str, Any],\n",
    "        embeddings: Optional[Any] = None,\n",
    "    ) -> TensorType:\n",
    "        if embeddings is None:\n",
    "            # Separate vf-encoder.\n",
    "            if hasattr(self.encoder, \"critic_encoder\"):\n",
    "                batch_ = batch\n",
    "                if self.is_stateful():\n",
    "                    # The recurrent encoders expect a `(state_in, h)`  key in the\n",
    "                    # input dict while the key returned is `(state_in, critic, h)`.\n",
    "                    batch_ = batch.copy()\n",
    "                    batch_[Columns.STATE_IN] = batch[Columns.STATE_IN][CRITIC]\n",
    "                embeddings = self.encoder.critic_encoder(batch_)[ENCODER_OUT]\n",
    "            # Shared encoder.\n",
    "            else:\n",
    "                embeddings = self.encoder(batch)[ENCODER_OUT][CRITIC]\n",
    "\n",
    "        # Value head.\n",
    "        vf_out = self.vf(embeddings)\n",
    "        # Squeeze out last dimension (single node value head).\n",
    "        return vf_out.squeeze(-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray_minicourse-PTDOXG61",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
