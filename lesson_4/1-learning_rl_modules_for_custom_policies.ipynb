{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding RLModules for custom policies in Ray RLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from ray.rllib.core.rl_module.rl_module import RLModuleSpec\n",
    "from typing import Any, Dict\n",
    "from ray.rllib.core.rl_module.torch.torch_rl_module import TorchRLModule\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ray RLlib defines [RLModules](https://docs.ray.io/en/latest/rllib/key-concepts.html) as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> RLModules are framework-specific neural network containers. In a nutshell, they carry the neural networks and define how to use them during three phases that occur in reinforcement learning: Exploration, inference and training. A minimal RL Module can contain a single neural network and define its exploration-, inference- and training logic to only map observations to actions. Since RL Modules can map observations to actions, they naturally implement reinforcement learning policies in RLlib and can therefore be found in the RolloutWorker, where their exploration and inference logic is used to sample from an environment. The second place in RLlib where RL Modules commonly occur is the Learner, where their training logic is used in training the neural network. RL Modules extend to the multi-agent case, where a single MultiRLModule contains multiple RL Modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarizing, the RLModules implement the neural networks our RL algorithms are going to use for training and inferencing. So, in order to implement our own neural network architectures and input/output policy dynamics, it is essential to understand RLModules. Figure below illustrates the RLModules being responsible for dealing the neural network inputs and outputs.\n",
    "\n",
    "![RLModules archtecture](./imgs/rl_module.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's investigate the RLModule structure by implementing a custom RLModule for a discrete action space based on Torch framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteTorchModule(TorchRLModule):  # We inherit from base class TorchRLModule\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space,\n",
    "        action_space,\n",
    "        inference_only,\n",
    "        model_config,\n",
    "        catalog_class,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            observation_space=observation_space,\n",
    "            action_space=action_space,\n",
    "            inference_only=inference_only,\n",
    "            model_config=model_config,\n",
    "            catalog_class=catalog_class,\n",
    "        )\n",
    "\n",
    "    def setup(self):\n",
    "        # Here we are going to create the policy network (neural network)\n",
    "        input_dim = self.observation_space.shape[\n",
    "            0\n",
    "        ]  # The neural network input dimension is the same as the observation space dimension\n",
    "        hidden_dim = self.model_config[\"fcnet_hiddens\"][\n",
    "            0\n",
    "        ]  # Represents the number of hidden units in the neural network\n",
    "        output_dim = (\n",
    "            self.action_space.n\n",
    "        )  # Finally, we have one neuron (output) per action in the action space\n",
    "\n",
    "        self.policy = nn.Sequential(  # Here we create the neural network using PyTorch\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "        )\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "    def _forward_inference(self, batch: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        # Forward pass for training.\n",
    "        with torch.no_grad():  # It disables gradient calculation and therefore no weight updates could be applied (and we don't want it since we are not training)\n",
    "            action = np.argmax(self.policy(batch[\"obs\"]))\n",
    "            return {\"actions\": action}\n",
    "\n",
    "    def _forward_exploration(self, batch: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        exploration_epsilon = 0.2\n",
    "        if np.random.rand() < exploration_epsilon:\n",
    "            # Random action exploration\n",
    "            action = np.random.choice(self.action_space.n)\n",
    "            return {\"actions\": action}\n",
    "        else:\n",
    "            # Use logits from policy network for action selection\n",
    "            logits = self.policy(batch[\"obs\"])\n",
    "            return {\n",
    "                \"action_dist_inputs\": torch.distributions.Categorical(logits=logits)\n",
    "            }\n",
    "\n",
    "    def _forward_train(self, batch: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        # Forward pass for training\n",
    "        action_logits = self.policy(\n",
    "            batch[\"obs\"]\n",
    "        )  # Here we pass the observation to the neural network and accounts the gradients\n",
    "        # Be aware that the gradients are accounted here because we are training\n",
    "\n",
    "        # When using a discrete action space, the neural network output is not the action itself, but the logits of the action distribution, therefore we need to apply a softmax function to get the discrete action. Therefore, we have one neural network output per action in the action space.\n",
    "        return {\n",
    "            \"action_dist_inputs\": torch.distributions.Categorical(logits=action_logits)\n",
    "        }  # You can read more about the Categorical distribution in the PyTorch documentation here https://pytorch.org/docs/stable/distributions.html#categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `RLModuleSpec` class to build our final RLModule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-14 03:33:43,698\tWARNING rl_module.py:427 -- Could not create a Catalog object for your RLModule! If you are not using the new API stack yet, make sure to switch it off in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. Some algos already use the new stack by default. Ignore this message, if your RLModule does not use a Catalog to build its sub-components.\n",
      "2024-12-14 03:33:43,699\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "spec = RLModuleSpec(\n",
    "    module_class=DiscreteTorchModule,\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    model_config={\"fcnet_hiddens\": [64]},\n",
    ")\n",
    "\n",
    "rlmodule = spec.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have a variable `rlmodule` containing a RLModule to interact with our neural network. It was not trained before, so the neural network weights have their initial values intact. Let's interact with it to verify how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inference does not explore new possibilities and, instead, always choose the action with the highest logit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs: tensor([1.3627, 0.1826, 0.1172, 0.1369])\n",
      "Action: 1\n"
     ]
    }
   ],
   "source": [
    "batch = {\"obs\": torch.from_numpy(env.observation_space.sample())}\n",
    "print(f\"Obs: {batch['obs']}\")\n",
    "\n",
    "# Forward inference\n",
    "inference = rlmodule.forward_inference(batch)\n",
    "inference_actions = inference[\"actions\"]\n",
    "print(f\"Action: {inference_actions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our exploration function explores random actions with a 20% of chance, otherwise it chooses the action with the highest logit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action dist: {'action_dist_inputs': Categorical(logits: torch.Size([2]))}\n",
      "Logits: tensor([-0.8223, -0.5788], grad_fn=<SubBackward0>)\n",
      "Probabilities: tensor([0.4394, 0.5606], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Forward exploration\n",
    "exploration = rlmodule.forward_exploration(batch)\n",
    "if \"action\" in exploration:\n",
    "    print(f\"Action: {exploration['actions']}\")\n",
    "elif \"action_dist_inputs\" in exploration:\n",
    "    exploration_action_dist_inputs = exploration[\"action_dist_inputs\"]\n",
    "    print(f\"Action dist: {exploration}\")\n",
    "    print(f\"Logits: {exploration_action_dist_inputs.logits}\")\n",
    "    print(f\"Probabilities: {exploration_action_dist_inputs.probs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training, we return the distribution of probabilities for each action with gradient calculation enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 1\n",
      "Logits: tensor([-0.8223, -0.5788], grad_fn=<SubBackward0>)\n",
      "Probabilities: tensor([0.4394, 0.5606], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Forward train\n",
    "train = rlmodule.forward_train(batch)\n",
    "train_action_dist_inputs = train[\"action_dist_inputs\"]\n",
    "print(f\"Action: {train_action_dist_inputs.sample()}\")\n",
    "print(f\"Logits: {train_action_dist_inputs.logits}\")\n",
    "print(f\"Probabilities: {train_action_dist_inputs.probs}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray_minicourse-PTDOXG61",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
