{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Learner module for guiding our neural network updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to [Ray RLlib definition](https://docs.ray.io/en/latest/rllib/rllib-learner.html), Learners allows you to abstract the training logic of RLModules. It supports both gradient-based and non-gradient-based updates (e.g. polyak averaging, etc.) The API enables you to distribute the Learner using data- distributed parallel (DDP). The Learner achieves the following:\n",
    "\n",
    "1. Facilitates gradient-based updates on RLModule.\n",
    "2. Provides abstractions for non-gradient based updates such as polyak averaging, etc.\n",
    "3. Reporting training statistics.\n",
    "4. Checkpoints the modules and optimizer states for durable training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to focus on the first point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.core.learner import Learner\n",
    "from ray.rllib.core.learner.torch.torch_learner import TorchLearner\n",
    "from ray.rllib.algorithms.ppo.torch.ppo_torch_learner import PPOTorchLearner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the RLModule was responsible for implementing the neural networks of our DRL system, the Learner is responsible for implementing the mechanisms to update our neural networks according to the used RL method (usually represented by the loss function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are adopting a different approach where instead of creating a custom Learner, we are going to analyze the current Proximal Policy Optimization (PPO) RL learner implementation from Ray RLlib. There is a [Learner base class](https://github.com/ray-project/ray/blob/master/rllib/core/learner/learner.py) from Ray RLlib which implement base methods for calculating the gradients based on a specific loss definition and update the RL module parameters (neural networks) based on that. There are a lot of information on this class, but you can try to focus on some functions such as the presence of abstract functions for `compute_gradients()`, `apply_gradients()`, and `compute_loss_for_module()` which are implemented in the subclasses of Learner base class to deal with the gradients and compute loss values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ray RLlib assumes two different frameworks (Pytorch and Tensorflow) to compute gradient operations for neural networks, and therefore there are two base classes called [TorchLearner](https://github.com/ray-project/ray/blob/master/rllib/core/learner/torch/torch_learner.py) and [TfLearner](https://github.com/ray-project/ray/blob/master/rllib/core/learner/tf/tf_learner.py) for implementing gradient operations for Pytorch and Tensorflow frameworks. You can check the `compute_gradients()` and `apply_gradients()` functions of these classes to verify the difference of computing and applying gradient updates using both frameworks. You can also check that the function `compute_loss_for_module()` is not implemented in any of these classes yet. Differently from `compute_gradients()` and `apply_gradients()` functions which are common operations for all the RL algorithms that deal with neural networks, the function `compute_loss_for_module()` is directly related to the adopted RL algorithm method, and therefore we should have a different implementation of this function when considering different algorithms such as PPO, SAC and other RL methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the information until this point are common for different RL methods, but now we will start to analyze the specific PPO Learner implemented in the class [PPOTorchLearner](https://github.com/ray-project/ray/blob/master/rllib/algorithms/ppo/torch/ppo_torch_learner.py) for the Pytorch framework. The Ray RLlib code for `PPOTorchLearner` is presented below. Remember from the function `compute_loss_for_module()` that was not implemented in the `TorchLearner`? It is implemented in the `PPOTorchLearner`. It is important to remember the PPO loss function to the Actor and Critic policies from the [paper](https://arxiv.org/abs/1707.06347)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Value Function (VF) loss (Critic loss) function is \n",
    "$$\n",
    "L_t^{\\mathsf{VF}}(W) = \\mathbb{E} \\left[(V_W(s[t])-R_t)^2\\right],\n",
    "$$ \n",
    "where $V_W(s[i])$ is the the critic output (vf policy output) which an approximation of the true return function, and $R_t$ is the return obtained in the batch of experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Actor loss function is\n",
    "$$\n",
    "L_{t}^{\\mathsf{clip}}(\\theta) = \\mathbb{\\hat E}_t \\!\\! \\left[ \\min ( r_t(\\theta) \\hat A_t, \\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat A_t ) \\right],\n",
    "$$\n",
    "where the probability ratio \n",
    "$$\n",
    "    r_t(\\theta)=\\frac{\\pi_{\\theta}(t(t)|s(t))}{\\pi_{\\theta\\mathsf{old}}(a(t)|s(t))}\n",
    "$$\n",
    "represents the current policy $\\pi_{\\theta}$ changes in relation to the old policy $\\pi_{\\theta\\mathsf{old}}$. And, $A_t$ is the generalized advantage estimation which represents how much better a particular action was compared to the average action at a given state $s(t)$. The epsilon $\\epsilon$ is a PPO hyperparameter. You can check the original paper for more information about the loss functions.\n",
    "\n",
    "**Here we are ignoring the KL contribution for simplicity**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's look at the PPO Learner implementation for Torch framework (class PPOTorchLearner) and let's try to find the presented loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Any, Dict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from ray.rllib.algorithms.ppo.ppo import (\n",
    "    LEARNER_RESULTS_KL_KEY,\n",
    "    LEARNER_RESULTS_CURR_KL_COEFF_KEY,\n",
    "    LEARNER_RESULTS_VF_EXPLAINED_VAR_KEY,\n",
    "    LEARNER_RESULTS_VF_LOSS_UNCLIPPED_KEY,\n",
    "    PPOConfig,\n",
    ")\n",
    "from ray.rllib.algorithms.ppo.ppo_learner import PPOLearner\n",
    "from ray.rllib.core.columns import Columns\n",
    "from ray.rllib.core.learner.learner import POLICY_LOSS_KEY, VF_LOSS_KEY, ENTROPY_KEY\n",
    "from ray.rllib.core.learner.torch.torch_learner import TorchLearner\n",
    "from ray.rllib.evaluation.postprocessing import Postprocessing\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from ray.rllib.utils.torch_utils import explained_variance\n",
    "from ray.rllib.utils.typing import ModuleID, TensorType\n",
    "\n",
    "torch, nn = try_import_torch()\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class PPOTorchLearner(PPOLearner, TorchLearner):\n",
    "    \"\"\"Implements torch-specific PPO loss logic on top of PPOLearner.\n",
    "\n",
    "    This class implements the ppo loss under `self.compute_loss_for_module()`.\n",
    "    \"\"\"\n",
    "\n",
    "    @override(TorchLearner)\n",
    "    def compute_loss_for_module(  # Here we define the loss function calculation\n",
    "        self,\n",
    "        *,\n",
    "        module_id: ModuleID,\n",
    "        config: PPOConfig,\n",
    "        batch: Dict[str, Any],\n",
    "        fwd_out: Dict[str, TensorType],\n",
    "    ) -> TensorType:\n",
    "        module = self.module[\n",
    "            module_id\n",
    "        ].unwrapped()  # That's the policy model (RLModule)\n",
    "\n",
    "        ###### Ignore these detail implementation for now ######\n",
    "        if Columns.LOSS_MASK in batch:\n",
    "            mask = batch[Columns.LOSS_MASK]\n",
    "            num_valid = torch.sum(mask)\n",
    "\n",
    "            def possibly_masked_mean(data_):\n",
    "                return torch.sum(data_[mask]) / num_valid\n",
    "\n",
    "        else:\n",
    "            possibly_masked_mean = torch.mean\n",
    "        ################################################\n",
    "\n",
    "        # Here we are getting the action distribution from the policy output for both the current and previous policy (before update neural network parameters)\n",
    "        action_dist_class_train = module.get_train_action_dist_cls()\n",
    "        action_dist_class_exploration = module.get_exploration_action_dist_cls()\n",
    "        curr_action_dist = action_dist_class_train.from_logits(\n",
    "            fwd_out[Columns.ACTION_DIST_INPUTS]\n",
    "        )\n",
    "        prev_action_dist = action_dist_class_exploration.from_logits(\n",
    "            batch[Columns.ACTION_DIST_INPUTS]\n",
    "        )\n",
    "\n",
    "        # Calculate the log probability ratio between the current and previous policy, that is our r_t(\\theta)\n",
    "        # You have to remember that a value A/B is the same as exp(log(A) - log(B))\n",
    "        logp_ratio = torch.exp(\n",
    "            curr_action_dist.logp(batch[Columns.ACTIONS]) - batch[Columns.ACTION_LOGP]\n",
    "        )\n",
    "\n",
    "        ##### Remember the loss for PPO can consider the KL contribution too? we are just ignoring it for now (config.use_kl_loss=False)\n",
    "        # Only calculate kl loss if necessary (kl-coeff > 0.0).\n",
    "        if config.use_kl_loss:\n",
    "            action_kl = prev_action_dist.kl(curr_action_dist)\n",
    "            mean_kl_loss = possibly_masked_mean(action_kl)\n",
    "        else:\n",
    "            mean_kl_loss = torch.tensor(0.0, device=logp_ratio.device)\n",
    "        #################################\n",
    "\n",
    "        # Usually the entropy is used to encourage exploration. I omitted it in the previous explanation to simplify the understanding.\n",
    "        curr_entropy = curr_action_dist.entropy()\n",
    "        mean_entropy = possibly_masked_mean(curr_entropy)\n",
    "\n",
    "        # Compute the surrogate loss, that's our L^{CLIP}(\\theta) in the PPO paper\n",
    "        surrogate_loss = torch.min(\n",
    "            batch[Postprocessing.ADVANTAGES] * logp_ratio,\n",
    "            batch[Postprocessing.ADVANTAGES]\n",
    "            * torch.clamp(logp_ratio, 1 - config.clip_param, 1 + config.clip_param),\n",
    "        )  # This is the clipped version of the surrogate loss\n",
    "\n",
    "        # Compute a value function loss.\n",
    "        if config.use_critic:\n",
    "            # Here we calculate our value function loss (L^{VF}(\\theta))\n",
    "            value_fn_out = module.compute_values(\n",
    "                batch, embeddings=fwd_out.get(Columns.EMBEDDINGS)\n",
    "            )\n",
    "            vf_loss = torch.pow(\n",
    "                value_fn_out - batch[Postprocessing.VALUE_TARGETS], 2.0\n",
    "            )  # Here we are calculating the squared error between the value function output and the target value\n",
    "            vf_loss_clipped = torch.clamp(\n",
    "                vf_loss, 0, config.vf_clip_param\n",
    "            )  # There is a parameter to also clip the value function loss\n",
    "            mean_vf_loss = possibly_masked_mean(vf_loss_clipped)\n",
    "            mean_vf_unclipped_loss = possibly_masked_mean(vf_loss)\n",
    "        # Ignore the value function -> Set all to 0.0.\n",
    "        else:\n",
    "            z = torch.tensor(0.0, device=surrogate_loss.device)\n",
    "            value_fn_out = mean_vf_unclipped_loss = vf_loss_clipped = mean_vf_loss = z\n",
    "\n",
    "        # Finally, out total loss (actor and critic loss functions, and entropy contribution together) is calculated here\n",
    "        total_loss = possibly_masked_mean(\n",
    "            -surrogate_loss\n",
    "            + config.vf_loss_coeff * vf_loss_clipped\n",
    "            - (\n",
    "                self.entropy_coeff_schedulers_per_module[module_id].get_current_value()\n",
    "                * curr_entropy\n",
    "            )\n",
    "        )\n",
    "\n",
    "        ###### We are ignoring the code below since it is related to KL contibution that we assume to be ignored by setting the use_kl_loss=False\n",
    "\n",
    "        # Add mean_kl_loss (already processed through `possibly_masked_mean`),\n",
    "        # if necessary.\n",
    "        if config.use_kl_loss:\n",
    "            total_loss += self.curr_kl_coeffs_per_module[module_id] * mean_kl_loss\n",
    "\n",
    "        # Log important loss stats.\n",
    "        self.metrics.log_dict(\n",
    "            {\n",
    "                POLICY_LOSS_KEY: -possibly_masked_mean(surrogate_loss),\n",
    "                VF_LOSS_KEY: mean_vf_loss,\n",
    "                LEARNER_RESULTS_VF_LOSS_UNCLIPPED_KEY: mean_vf_unclipped_loss,\n",
    "                LEARNER_RESULTS_VF_EXPLAINED_VAR_KEY: explained_variance(\n",
    "                    batch[Postprocessing.VALUE_TARGETS], value_fn_out\n",
    "                ),\n",
    "                ENTROPY_KEY: mean_entropy,\n",
    "                LEARNER_RESULTS_KL_KEY: mean_kl_loss,\n",
    "            },\n",
    "            key=module_id,\n",
    "            window=1,  # <- single items (should not be mean/ema-reduced over time).\n",
    "        )\n",
    "        # Return the total loss.\n",
    "        return total_loss\n",
    "\n",
    "    @override(PPOLearner)\n",
    "    def _update_module_kl_coeff(\n",
    "        self,\n",
    "        *,\n",
    "        module_id: ModuleID,\n",
    "        config: PPOConfig,\n",
    "        kl_loss: float,\n",
    "    ) -> None:\n",
    "        if np.isnan(kl_loss):\n",
    "            logger.warning(\n",
    "                f\"KL divergence for Module {module_id} is non-finite, this \"\n",
    "                \"will likely destabilize your model and the training \"\n",
    "                \"process. Action(s) in a specific state have near-zero \"\n",
    "                \"probability. This can happen naturally in deterministic \"\n",
    "                \"environments where the optimal policy has zero mass for a \"\n",
    "                \"specific action. To fix this issue, consider setting \"\n",
    "                \"`kl_coeff` to 0.0 or increasing `entropy_coeff` in your \"\n",
    "                \"config.\"\n",
    "            )\n",
    "\n",
    "        # Update the KL coefficient.\n",
    "        curr_var = self.curr_kl_coeffs_per_module[module_id]\n",
    "        if kl_loss > 2.0 * config.kl_target:\n",
    "            # TODO (Kourosh) why not 2?\n",
    "            curr_var.data *= 1.5\n",
    "        elif kl_loss < 0.5 * config.kl_target:\n",
    "            curr_var.data *= 0.5\n",
    "\n",
    "        # Log the updated KL-coeff value.\n",
    "        self.metrics.log_value(\n",
    "            (module_id, LEARNER_RESULTS_CURR_KL_COEFF_KEY),\n",
    "            curr_var.item(),\n",
    "            window=1,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After explaining the PPO Learner in details and how it implements the paper theory on Ray RLlib, you have now knowledge to implement different Loss functions for PPO in order to improve its performance -- or find out the defined loss function is very good and hard to outperform :) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray_minicourse-PTDOXG61",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
