{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Algorithm class to understand the PPO RL implementation on Ray RLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the PPO architecture displayed in Ray RLlib (image below), we can verify the Learner interact with RLModule to optimize the policies (neural networks). But, the EnvRunners and the training_step mechanisms are not implemented in the two previous structures, RLModules and Learners, we learned about. The EnvRunner is responsible for using frozen policies (policies that are not updating their parameters) to interact with the environment providing actions and receiving observations and rewards which compose our batches of experiences to be used by the Learners to update the Policy parameters. Therefore, the PPO implementation inherits from an Algorithm class, and they are responsible for coordinating the EnvRunners and Learners to optimize the policy parameters through the interaction with the environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo.ppo import PPOConfig, PPO\n",
    "from ray.rllib.algorithms import AlgorithmConfig, Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PPO architecture](./imgs/ray_rllib_ppo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Algorithm class is the base class for different RL algorithms implementation (You can check Algorithm class code [here](https://github.com/ray-project/ray/blob/master/rllib/algorithms/algorithm.py)). You can take a look in the function `setup()` which is the responsbile to read the `AlgorithmConfig` class containing algorithm specific configurations and create the EnvRunners and Learners that compose the learning process. You can also check the functions `training_step()`, and `compute_actions()`.\n",
    "\n",
    "The `training_step()` function is responsible for implementing a single iteration logic of the algorithm and is usually overwritten by the RL algorithm logic that is going to inherit it. The code for general `training_step()` from `Algorithm` class is presented below. It is divided in three groups of steps, the first one collects batches of experience from the environments using a frozen policy in the EnvRunners. The second part uses the Learners to update/optimize the policies based on collected experiences. The third part synchronize the new parameters obtained for policies through Learner operations with the policies from EnvRunners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@OverrideToImplementCustomLogic\n",
    "def training_step(self) -> None:\n",
    "    \"\"\"Default single iteration logic of an algorithm.\n",
    "\n",
    "    - Collect on-policy samples (SampleBatches) in parallel using the\n",
    "            Algorithm's EnvRunners (@ray.remote).\n",
    "    - Concatenate collected SampleBatches into one train batch.\n",
    "    - Note that we may have more than one policy in the multi-agent case:\n",
    "            Call the different policies' `learn_on_batch` (simple optimizer) OR\n",
    "            `load_batch_into_buffer` + `learn_on_loaded_batch` (multi-GPU\n",
    "            optimizer) methods to calculate loss and update the model(s).\n",
    "    - Return all collected metrics for the iteration.\n",
    "\n",
    "    Returns:\n",
    "            For the new API stack, returns None. Results are compiled and extracted\n",
    "            automatically through a single `self.metrics.reduce()` call at the very end\n",
    "            of an iteration (which might contain more than one call to\n",
    "            `training_step()`). This way, we make sure that we account for all\n",
    "            results generated by each individual `training_step()` call.\n",
    "            For the old API stack, returns the results dict from executing the training\n",
    "            step.\n",
    "    \"\"\"\n",
    "    if not self.config.enable_env_runner_and_connector_v2:\n",
    "        raise NotImplementedError(\n",
    "            \"The `Algorithm.training_step()` default implementation no longer \"\n",
    "            \"supports the old API stack! If you would like to continue \"\n",
    "            \"using these \"\n",
    "            \"old APIs with this default `training_step`, simply subclass \"\n",
    "            \"`Algorithm` and override its `training_step` method (copy/paste the \"\n",
    "            \"code and delete this error message).\"\n",
    "        )\n",
    "\n",
    "    # Collect a list of Episodes from EnvRunners until we reach the train batch\n",
    "    # size.\n",
    "    with self.metrics.log_time((TIMERS, ENV_RUNNER_SAMPLING_TIMER)):\n",
    "        if self.config.count_steps_by == \"agent_steps\":\n",
    "            episodes, env_runner_results = synchronous_parallel_sample(\n",
    "                worker_set=self.env_runner_group,\n",
    "                max_agent_steps=self.config.total_train_batch_size,\n",
    "                sample_timeout_s=self.config.sample_timeout_s,\n",
    "                _uses_new_env_runners=True,\n",
    "                _return_metrics=True,\n",
    "            )\n",
    "        else:\n",
    "            episodes, env_runner_results = synchronous_parallel_sample(\n",
    "                worker_set=self.env_runner_group,\n",
    "                max_env_steps=self.config.total_train_batch_size,\n",
    "                sample_timeout_s=self.config.sample_timeout_s,\n",
    "                _uses_new_env_runners=True,\n",
    "                _return_metrics=True,\n",
    "            )\n",
    "    # Reduce EnvRunner metrics over the n EnvRunners.\n",
    "    self.metrics.merge_and_log_n_dicts(env_runner_results, key=ENV_RUNNER_RESULTS)\n",
    "\n",
    "    # Here, the Learners utilize the batches of experiences to update/optimize the policy parameters\n",
    "    with self.metrics.log_time((TIMERS, LEARNER_UPDATE_TIMER)):\n",
    "        learner_results = self.learner_group.update_from_episodes(\n",
    "            episodes=episodes,\n",
    "            timesteps={\n",
    "                NUM_ENV_STEPS_SAMPLED_LIFETIME: (\n",
    "                    self.metrics.peek(NUM_ENV_STEPS_SAMPLED_LIFETIME)\n",
    "                ),\n",
    "            },\n",
    "        )\n",
    "        self.metrics.log_dict(learner_results, key=LEARNER_RESULTS)\n",
    "\n",
    "    # Update weights - after learning on the local worker - on all\n",
    "    # remote workers (only those RLModules that were actually trained).\n",
    "    with self.metrics.log_time((TIMERS, SYNCH_WORKER_WEIGHTS_TIMER)):\n",
    "        self.env_runner_group.sync_weights(\n",
    "            from_worker_or_learner_group=self.learner_group,\n",
    "            policies=list(set(learner_results.keys()) - {ALL_MODULES}),\n",
    "            inference_only=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL algorithm logics usually can change this `Algorithm` class default logic for `training_step()` function and that is why they override it. For instance, let's take a look at the `PPO` class implementation that inherits from `Algorithm` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO(Algorithm):\n",
    "    @classmethod\n",
    "    @override(Algorithm)\n",
    "    def get_default_config(cls) -> AlgorithmConfig:\n",
    "        return PPOConfig()\n",
    "\n",
    "    @classmethod\n",
    "    @override(Algorithm)\n",
    "    def get_default_policy_class(\n",
    "        cls, config: AlgorithmConfig\n",
    "    ) -> Optional[Type[Policy]]:\n",
    "        if config[\"framework\"] == \"torch\":\n",
    "\n",
    "            from ray.rllib.algorithms.ppo.ppo_torch_policy import PPOTorchPolicy\n",
    "\n",
    "            return PPOTorchPolicy\n",
    "        elif config[\"framework\"] == \"tf\":\n",
    "            from ray.rllib.algorithms.ppo.ppo_tf_policy import PPOTF1Policy\n",
    "\n",
    "            return PPOTF1Policy\n",
    "        else:\n",
    "            from ray.rllib.algorithms.ppo.ppo_tf_policy import PPOTF2Policy\n",
    "\n",
    "            return PPOTF2Policy\n",
    "\n",
    "    @override(Algorithm)\n",
    "    def training_step(self) -> None:\n",
    "        # Old API stack (Policy, RolloutWorker, Connector).\n",
    "        if not self.config.enable_env_runner_and_connector_v2:\n",
    "            return self._training_step_old_api_stack()\n",
    "\n",
    "        # Collect batches from sample workers until we have a full batch.\n",
    "        with self.metrics.log_time((TIMERS, ENV_RUNNER_SAMPLING_TIMER)):\n",
    "            # Sample in parallel from the workers.\n",
    "            if self.config.count_steps_by == \"agent_steps\":\n",
    "                episodes, env_runner_results = synchronous_parallel_sample(\n",
    "                    worker_set=self.env_runner_group,\n",
    "                    max_agent_steps=self.config.total_train_batch_size,\n",
    "                    sample_timeout_s=self.config.sample_timeout_s,\n",
    "                    _uses_new_env_runners=(\n",
    "                        self.config.enable_env_runner_and_connector_v2\n",
    "                    ),\n",
    "                    _return_metrics=True,\n",
    "                )\n",
    "            else:\n",
    "                episodes, env_runner_results = synchronous_parallel_sample(\n",
    "                    worker_set=self.env_runner_group,\n",
    "                    max_env_steps=self.config.total_train_batch_size,\n",
    "                    sample_timeout_s=self.config.sample_timeout_s,\n",
    "                    _uses_new_env_runners=(\n",
    "                        self.config.enable_env_runner_and_connector_v2\n",
    "                    ),\n",
    "                    _return_metrics=True,\n",
    "                )\n",
    "            # Return early if all our workers failed.\n",
    "            if not episodes:\n",
    "                return\n",
    "\n",
    "            # Reduce EnvRunner metrics over the n EnvRunners.\n",
    "            self.metrics.merge_and_log_n_dicts(\n",
    "                env_runner_results, key=ENV_RUNNER_RESULTS\n",
    "            )\n",
    "\n",
    "        # Perform a learner update step on the collected episodes.\n",
    "        with self.metrics.log_time((TIMERS, LEARNER_UPDATE_TIMER)):\n",
    "            learner_results = self.learner_group.update_from_episodes(\n",
    "                episodes=episodes,\n",
    "                timesteps={\n",
    "                    NUM_ENV_STEPS_SAMPLED_LIFETIME: (\n",
    "                        self.metrics.peek(\n",
    "                            (ENV_RUNNER_RESULTS, NUM_ENV_STEPS_SAMPLED_LIFETIME)\n",
    "                        )\n",
    "                    ),\n",
    "                },  ########## Here is the difference from Algorithm.training_step(), The three following parameters are not used in the Algorithm class\n",
    "                num_epochs=self.config.num_epochs,\n",
    "                minibatch_size=self.config.minibatch_size,\n",
    "                shuffle_batch_per_epoch=self.config.shuffle_batch_per_epoch,\n",
    "            )\n",
    "            self.metrics.merge_and_log_n_dicts(learner_results, key=LEARNER_RESULTS)\n",
    "\n",
    "        # Update weights - after learning on the local worker - on all remote\n",
    "        # workers.\n",
    "        with self.metrics.log_time((TIMERS, SYNCH_WORKER_WEIGHTS_TIMER)):\n",
    "            modules_to_update = set(learner_results[0].keys()) - {ALL_MODULES}\n",
    "            self.env_runner_group.sync_weights(\n",
    "                # Sync weights from learner_group to all EnvRunners.\n",
    "                from_worker_or_learner_group=self.learner_group,\n",
    "                policies=modules_to_update,\n",
    "                inference_only=True,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, we don't handle the `Algorithm` class or the RL algorithm class (such as `PPO` class) directly, instead, we utilize the `AlgorithmConfig` class or the `PPOConfig` class (which inherits form `AlgorithmConfig` class) to configure our Algorithm with the hyperparameters and configuration of our choice. The general `AlgorithmConfig` class contains the general configuration which are habitually considered in most of the RL agents and also configurations related to the Ray RLlib execution, such as `num_cpus_per_env_runner` and learning rate `lr`. You can check the `AlgorithmConfig` details [here](https://github.com/ray-project/ray/blob/master/rllib/algorithms/algorithm_config.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `PPOConfig` class inherits from `AlgorithmConfig` class and implement PPO specific parameters that should be utilized to execute the PPO RL algorithm. You can also check the `PPOConfig` implementation [here](https://github.com/ray-project/ray/blob/master/rllib/algorithms/ppo/ppo.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finalize, do you remember how did we implement the PPO agent in the previous lessons? The code below shows an example of [Lesson 1 notebook 3](../lesson_1/3-rl_agent_for_radio_resource_scheduler.ipynb). We utilized the PPOConfig class to configure and build our PPO agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(\"comm_env\")\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .learners(num_learners=1)\n",
    "    .training(\n",
    "        lr=0.0003,\n",
    "        train_batch_size=2048,\n",
    "        sgd_minibatch_size=64,\n",
    "        num_sgd_iter=10,\n",
    "        gamma=0.99,\n",
    "        lambda_=0.95,\n",
    "        model={\n",
    "            \"fcnet_hiddens\": [64, 64],\n",
    "            \"fcnet_activation\": \"relu\",\n",
    "        },\n",
    "        clip_param=0.2,\n",
    "        entropy_coeff=0.01,\n",
    "        vf_loss_coeff=0.5,\n",
    "        grad_clip=0.5,\n",
    "        vf_clip_param=np.inf,\n",
    "        use_gae=True,\n",
    "        kl_coeff=0,\n",
    "        use_kl_loss=False,\n",
    "        kl_target=0,\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray_minicourse-PTDOXG61",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
